{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "import textract\n",
    "import nltk\n",
    "\n",
    "# MODIFY THESE LINES TO POINT TO YOUR WORD OR PDF FILE\n",
    "str1 = '/Users/prajwalseth/Desktop/Ashoka/sem3/Mind and Behaviour/Week 1/Monday/'\n",
    "str2 = \"Rosen.pdf\"\n",
    "\n",
    "\n",
    "str = str1+str2\n",
    "text = textract.process(str)\n",
    "\n",
    "from readability.readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "readable_article = Document(text).summary()\n",
    "readable_title = Document(text).title()\n",
    "soup = BeautifulSoup((text), \"lxml\")\n",
    "\n",
    "tokens = [word for sent in nltk.sent_tokenize(soup.text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "import datetime, re, sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "for article in text:\n",
    "    token_dict[article] = soup.text\n",
    "        \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
    "sys.stdout.flush()\n",
    "\n",
    "tdm = tfidf.fit_transform(token_dict.values()) # this can take some time (about 60 seconds on my machine)\n",
    "\n",
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "import math\n",
    "from __future__ import division\n",
    "import collections\n",
    "\n",
    "article_id = 0\n",
    "#article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "\n",
    "article_text = soup.text\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    \n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    \n",
    "    if len(sent_tokens) ==0:\n",
    "        continue;\n",
    "    \n",
    "    else:\n",
    "        sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / (3)))\n",
    "var = 0\n",
    "order = {}\n",
    "sent_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "print ('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    var = article_text.find(summary_sentence[1])\n",
    "    order[var] = summary_sentence[1]\n",
    "    # print (summary_sentence[1])\n",
    "ordered = collections.OrderedDict(sorted(order.items()))\n",
    "for k, v in ordered.items(): \n",
    "    print(v) #end = \" \")\n",
    "print('\\n')\n",
    "print ('\\n*** ORIGINAL ***')\n",
    "print (article_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
